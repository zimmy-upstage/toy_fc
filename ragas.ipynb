{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersection(str1, str2):\n",
    "    # Check for inclusion first (simpler case)\n",
    "    if str1 in str2:\n",
    "        return str1\n",
    "    if str2 in str1:\n",
    "        return str2\n",
    "        \n",
    "    # Find the longest matching substring between end of str1/start of str2 \n",
    "    # or end of str2/start of str1\n",
    "    len1, len2 = len(str1), len(str2)\n",
    "    for i in range(1, min(len1, len2) + 1):\n",
    "        if str1[-i:] == str2[:i]:\n",
    "            return str1[-i:]\n",
    "        if str2[-i:] == str1[:i]:\n",
    "            return str2[-i:]\n",
    "            \n",
    "    return None\n",
    "\n",
    "def find_intersection_ratio(response, reference):\n",
    "    intersection = find_intersection(response, reference)\n",
    "    if intersection:\n",
    "        splitted_response = response.split()\n",
    "        splitted_reference = reference.split()\n",
    "        splitted_intersection = intersection.split()\n",
    "        \n",
    "        min_ratio = min(len(splitted_intersection) / len(splitted_response), len(splitted_intersection) / len(splitted_reference))\n",
    "    else:\n",
    "        min_ratio = 0\n",
    "\n",
    "    return min_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_claims = [\n",
    "    \"Using notably fewer parameters, Solar Mini delivers better responses than GPT-3.5 and is at least 5 times faster.\",\n",
    "    \"delivers better responses than GPT-3.5 and is at least 5 times faster.\",\n",
    "    \"better responses than GPT-3.5\",\n",
    "    \"carry better responses than GPT-3.5\",\n",
    "    \"better responses than GPT-3.5 with a smaller model size\",\n",
    "]\n",
    "gt_claim = [\"Solar Mini delivers better responses than GPT-3.5 and is at least 5 times faster.\"]\n",
    "\n",
    "# \"Solar Mini is perfectly optimized for all RAG systems and always provides the most accurate responses.\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n",
      "0.8571428571428571\n",
      "0.2857142857142857\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "all_combinations = product(output_claims, gt_claim)\n",
    "\n",
    "for response, reference in all_combinations:\n",
    "    print(find_intersection_ratio(response, reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll test cases passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Run tests\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtest_intersection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 39\u001b[0m, in \u001b[0;36mtest_intersection\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Test case 1\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m find_list_intersection(list1, list2) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m find_list_intersection(list2, list1) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Test case 2\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m find_list_intersection(list1, list3) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def find_list_intersection(list1, list2):\n",
    "    \"\"\"\n",
    "    Find the intersection of two lists where elements from the end of list1 \n",
    "    match with the start of list2, preserving order.\n",
    "    \n",
    "    Args:\n",
    "        list1: First list\n",
    "        list2: Second list\n",
    "    \n",
    "    Returns:\n",
    "        List of matching elements in order, or None if no match found\n",
    "    \"\"\"\n",
    "    if not list1 or not list2:\n",
    "        return None\n",
    "        \n",
    "    # Find the maximum possible length of intersection\n",
    "    max_length = min(len(list1), len(list2))\n",
    "    \n",
    "    # Try different lengths of subsequences\n",
    "    for length in range(max_length, 0, -1):\n",
    "        # Get end of list1 and start of list2\n",
    "        end_of_list1 = list1[-length:]\n",
    "        start_of_list2 = list2[:length]\n",
    "        \n",
    "        # Check if sequences match\n",
    "        if end_of_list1 == start_of_list2:\n",
    "            return end_of_list1\n",
    "            \n",
    "    return None\n",
    "\n",
    "# Test cases\n",
    "def test_intersection():\n",
    "    list1 = [1, 2, 3, 4, 5]\n",
    "    list2 = [4, 5, 6, 7, 8]\n",
    "    list3 = [5, 4, 6, 7, 8]\n",
    "    \n",
    "    # Test case 1\n",
    "    assert find_list_intersection(list1, list2) == [4, 5]\n",
    "    assert find_list_intersection(list2, list1) == [4, 5]\n",
    "    \n",
    "    # Test case 2\n",
    "    assert find_list_intersection(list1, list3) == [5]\n",
    "    assert find_list_intersection(list3, list1) == [5]\n",
    "    \n",
    "    # Test case 3\n",
    "    assert find_list_intersection(list2, list3) == None\n",
    "    assert find_list_intersection(list3, list2) == None\n",
    "    \n",
    "    print(\"All test cases passed!\")\n",
    "\n",
    "# Run tests\n",
    "test_intersection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import RougeScore\n",
    "\n",
    "\n",
    "response_all = [\n",
    "    \"Using notably fewer parameters, Solar Mini delivers better responses than GPT-3.5 and is at least 5 times faster.\",\n",
    "    \"delivers better responses than GPT-3.5 and is at least 5 times faster.\",\n",
    "]\n",
    "reference_all = [\n",
    "    \"Solar Mini delivers better responses than GPT-3.5 and is at least 5 times faster.\",\n",
    "    \"Solar Mini is perfectly optimized for all RAG systems and always provides the most accurate responses.\",\n",
    "]\n",
    "\n",
    "# all product of response and reference\n",
    "from itertools import product\n",
    "\n",
    "sample_list = [\n",
    "    {\"response\": response, \"reference\": reference}\n",
    "    for response, reference in product(response_all, reference_all)\n",
    "]\n",
    "\n",
    "\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "eval_dataset = EvaluationDataset.from_list(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_type = \"fmeasure\" #fmeasure, recall, precision\n",
    "rouge_type = \"rougeL\"\n",
    "\n",
    "scorer = RougeScore(measure_type=measure_type, rogue_type=rouge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 1733.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "metrics = [\n",
    "    scorer\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge_score': 0.888888888888889},\n",
       " {'rouge_score': 0.16666666666666663},\n",
       " {'rouge_score': 0.9333333333333333},\n",
       " {'rouge_score': 0.06666666666666667}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge_score': 0.5139}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    [\n",
    "        {\n",
    "            \"response\": response_short,\n",
    "            \"reference\": reference\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
