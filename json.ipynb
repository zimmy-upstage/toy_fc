{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Extract statements to fact-check based on specific criteria and assign them a tag indicating their suitability for fact-checking.\n",
    "\n",
    "Consider the following criteria for each statement:\n",
    "1. **Verifiable**: Is the statement rooted in a fact that can be verified? Avoid statements that are opinions, hyperboles, or exaggerated rhetorical statements.\n",
    "2. **Misleading**: Does the statement seem misleading or sound incorrect in any way?\n",
    "3. **Significant**: Is the statement important or impactful? Avoid minor slip-ups or insignificant errors.\n",
    "4. **Viral**: Is the statement likely to be passed on and repeated by others?\n",
    "5. **Curiosity**: Would a typical person hear or read the statement and wonder or question if it's true?\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Extract Statements**: Carefully identify and extract verbatim statements eligible for fact-checking.\n",
    "2. **Assess and Tag**: Evaluate each extracted statement against the above criteria and assign tags.\n",
    "\n",
    "# Output Format\n",
    "\n",
    "- Each extracted statement should be followed by its assigned tag in JSON format.\n",
    "- Give tags from [\"Verifiable\", \"Misleading\", \"Significant\", \"Viral\", \"Curiosity\"]\n",
    "- Json structure: array of objects with keys \"statement\" and \"tags\" like below\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"statement\": #YOUR_STATEMENT#,\n",
    "        \"tags\": #YOUR_TAGS#\n",
    "    },\n",
    "    {\n",
    "        \"statement\": #YOUR_STATEMENT#,\n",
    "        \"tags\": #YOUR_TAGS#\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "# Examples\n",
    "\n",
    "**Input:** \n",
    "\"The unemployment rate is the lowest it's been in 50 years. In contrast, the stock market is bad.\"\n",
    "\n",
    "**Output:** \n",
    "[{\"statement\": \"The unemployment rate is the lowest it's been in 50 years.\", \"tags\": [\"Verifiable\", \"Viral\"]}, {\"statement\": \"In contrast, the stock market is bad.\", \"tags\": [\"Misleading\", \"Significant\", \"Curiosity\"]}]\n",
    "\n",
    "# Notes\n",
    "\n",
    "- Focus on statements that fulfill multiple criteria for a more robust fact-checking process.\n",
    "- Ensure that opinion-based statements are not extracted for fact-checking purposes.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Now, extract as many claims as possible from the following text:\n",
    "\n",
    "# Passage\n",
    "{input_text}\"\"\"\n",
    "\n",
    "BLOG = \"\"\"Introducing Solar Mini : Compact yet Powerful Product 2024/01/25 | Written By: Eujeong Choi (Technical Writer) The best model you’ll find under 30B Welcome to the era of Solar, a pre-trained Large Language Model (LLM) from Upstage. In December 2023, Solar Mini made waves by reaching the pinnacle of the Open LLM Leaderboard of Hugging Face. Using notably fewer parameters, Solar Mini delivers responses comparable to GPT-3.5, but is 2.5 times faster. Let me guide you through how Solar Mini revolutionized the downsizing of LLM models without sacrificing its performance. Looking into the model : Why we need Smaller LLMs Size became a pivotal factor in integrating Large Language Models (LLMs) into real-world applications. The main advantage of smaller models is their reduced computational time, which boosts responsiveness and efficiency. This translates to lower manpower requirements for optimization, as these LLMs are more straightforward to customize for specific domains and services. Additionally, their compact size enables on-device deployment, facilitating a decentralized approach that brings AI capabilities directly to the user's local device. This not only enhances accessibility but also diminishes the dependence on extensive GPU resources, paving the way for more new and affordable AI solutions. Speed of Solar Compact Size, Mighty Performance Solar Mini is proof that you don't need a large size for exceptional performance. It impressively outshined competitors like Llama2, Mistral 7B, Ko-Alpaca, and KULLM in a range of benchmarks. Evaluation results for Solar 10.7B and Solar 10.7B-Instruct along with other top-performing models. (Source: Solar 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling) Building Solar Mini Fundamentals The foundational architecture of Solar Mini is based on a 32-layer Llama 2 structure, and is initialized with pre-trained weights from Mistral 7B, one of the best-performing models compatible with the Llama 2 architecture. Depth Up-scaling (DUS) How did Solar Mini Solar Mini stay compact, yet become remarkably powerful? Our scaling method ‘depth up-scaling’ (DUS) consists of depthwise scaling and continued pretraining. DUS allows for a much more straightforward and efficient enlargement of smaller models than other scaling methods such as mixture-of-experts. Unlike Mixture of Experts (MoE), DUS doesn’t need complex changes. We don’t need additional modules or dynamism; DUS is immediately compatible with easy-to-use LLM frameworks such as HuggingFace, and is applicable to all transformer architectures. (Read paper → ) Continued Pre-training After depth up scaling, the model performs worse than the base LLM. Therefore, a continued pretraining stage is applied to recover the performance of the scaled model. Instruction Tuning In this stage, the model undergoes instruction tuning specifically for Korean, where it is trained to follow instructions in a QA (Question and Answer) format. Alignment Tuning In this stage, the instruction-tuned model is trained to align with human or powerful AI preferences. Use Solar with high-end components RAG Solar Mini especially works well with RAG systems. As LLMs get bigger, LLMs rely more on the pre-trained, parametric knowledge to answer your questions. Solar Mini effectively utilizes RAG to augment the precision and relevance of the output, thereby reinforcing its accuracy and reliability. Layout Analysis We have models that extract tables and figures from any document you have. Your PDF, PNG, JPG data are all covered through our OCR and Layout Analysis module. By serializing elements based on reading order and converting the output to HTML, it becomes a ready-to-go input into the LLM. Solar Mini is publicly available under Apache 2.0 license. For more : Read Paper / Try it on Hugging Face / Try it on Poe\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "\n",
    "\n",
    "def test_solar_json_mode(messages):\n",
    "    url = \"https://api.upstage.ai/v1/solar/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {UPSTAGE_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\",\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"solar-pro\",\n",
    "        # \"response_format\": {\n",
    "        #     \"type\": \"json_object\"\n",
    "        # },\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"content\": SYSTEM_PROMPT, \"role\": \"system\"},\n",
    "    {\"content\": USER_PROMPT.format(input_text=BLOG), \"role\": \"user\"},\n",
    "]\n",
    "res = test_solar_json_mode(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Now, extract as many claims as possible from the following text:\\n\\n# Passage\\nIntroducing Solar Mini : Compact yet Powerful Product 2024/01/25 | Written By: Eujeong Choi (Technical Writer) The best model you’ll find under 30B Welcome to the era of Solar, a pre-trained Large Language Model (LLM) from Upstage. In December 2023, Solar Mini made waves by reaching the pinnacle of the Open LLM Leaderboard of Hugging Face. Using notably fewer parameters, Solar Mini delivers responses comparable to GPT-3.5, but is 2.5 times faster. Let me guide you through how Solar Mini revolutionized the downsizing of LLM models without sacrificing its performance. Looking into the model : Why we need Smaller LLMs Size became a pivotal factor in integrating Large Language Models (LLMs) into real-world applications. The main advantage of smaller models is their reduced computational time, which boosts responsiveness and efficiency. This translates to lower manpower requirements for optimization, as these LLMs are more straightforward to customize for specific domains and services. Additionally, their compact size enables on-device deployment, facilitating a decentralized approach that brings AI capabilities directly to the user's local device. This not only enhances accessibility but also diminishes the dependence on extensive GPU resources, paving the way for more new and affordable AI solutions. Speed of Solar Compact Size, Mighty Performance Solar Mini is proof that you don't need a large size for exceptional performance. It impressively outshined competitors like Llama2, Mistral 7B, Ko-Alpaca, and KULLM in a range of benchmarks. Evaluation results for Solar 10.7B and Solar 10.7B-Instruct along with other top-performing models. (Source: Solar 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling) Building Solar Mini Fundamentals The foundational architecture of Solar Mini is based on a 32-layer Llama 2 structure, and is initialized with pre-trained weights from Mistral 7B, one of the best-performing models compatible with the Llama 2 architecture. Depth Up-scaling (DUS) How did Solar Mini Solar Mini stay compact, yet become remarkably powerful? Our scaling method ‘depth up-scaling’ (DUS) consists of depthwise scaling and continued pretraining. DUS allows for a much more straightforward and efficient enlargement of smaller models than other scaling methods such as mixture-of-experts. Unlike Mixture of Experts (MoE), DUS doesn’t need complex changes. We don’t need additional modules or dynamism; DUS is immediately compatible with easy-to-use LLM frameworks such as HuggingFace, and is applicable to all transformer architectures. (Read paper → ) Continued Pre-training After depth up scaling, the model performs worse than the base LLM. Therefore, a continued pretraining stage is applied to recover the performance of the scaled model. Instruction Tuning In this stage, the model undergoes instruction tuning specifically for Korean, where it is trained to follow instructions in a QA (Question and Answer) format. Alignment Tuning In this stage, the instruction-tuned model is trained to align with human or powerful AI preferences. Use Solar with high-end components RAG Solar Mini especially works well with RAG systems. As LLMs get bigger, LLMs rely more on the pre-trained, parametric knowledge to answer your questions. Solar Mini effectively utilizes RAG to augment the precision and relevance of the output, thereby reinforcing its accuracy and reliability. Layout Analysis We have models that extract tables and figures from any document you have. Your PDF, PNG, JPG data are all covered through our OCR and Layout Analysis module. By serializing elements based on reading order and converting the output to HTML, it becomes a ready-to-go input into the LLM. Solar Mini is publicly available under Apache 2.0 license. For more : Read Paper / Try it on Hugging Face / Try it on Poe\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER_PROMPT.format(input_text=BLOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "# Parse the JSON string into a Python dictionary\n",
    "data = json.loads(res[\"choices\"][0][\"message\"][\"content\"])\n",
    "dumps = json.dumps(data, indent=2)\n",
    "print(dumps, file=open(\"share_1.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "MORE_REQUEST_PROMPT = \"\"\"Good. Extract more claims as many as possible in the same way.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'statement': 'The main advantage of smaller models is their reduced computational time, which translates to lower manpower requirements for optimization.',\n",
       "  'tags': ['Verifiable', 'Significative']},\n",
       " {'statement': 'Smaller models are more straightforward to customize for specific domains and services.',\n",
       "  'tags': ['Verifiable', 'Significative']},\n",
       " {'statement': \"Their compact size enables on-device deployment, facilitating a decentralized approach that brings AI capabilities directly to the user's local device.\",\n",
       "  'tags': ['Verifiable', 'Significative']},\n",
       " {'statement': 'Solar Mini outshined competitors like Llama2, Mistral 7B, Ko-Alpaca, and KULLM in a range of benchmarks.',\n",
       "  'tags': ['Verifiable', 'Misleading', 'Significative', 'Viral']},\n",
       " {'statement': 'Depth up-scaling (DUS) consists of depthwise scaling and continued pretraining.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Depth up-scaling (DUS) allows for a much more straightforward and efficient enlargement of smaller models than other scaling methods such as mixture-of-experts.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': \"Depth up-scaling (DUS) doesn't need complex changes and doesn't need additional modules or dynamism.\",\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Depth up-scaling (DUS) is immediately compatible with easy-to-use LLM frameworks such as HuggingFace, and is applicable to all transformer architectures.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Continued pretraining stage is applied to recover the performance of the scaled model after depth up scaling.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Instruction tuning stage is where the model is trained to follow instructions in a QA format.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Alignment tuning stage is where the instruction-tuned model is trained to align with human or powerful AI preferences.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Solar Mini especially works well with RAG systems.',\n",
       "  'tags': ['Verifiable', 'Significative']},\n",
       " {'statement': 'Solar Mini effectively utilizes RAG to augment the precision and relevance of the output.',\n",
       "  'tags': ['Verifiable', 'Significative']},\n",
       " {'statement': 'Solar Mini is publicly available under Apache 2.0 license.',\n",
       "  'tags': ['Verifiable', 'Viral']}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_messages = [\n",
    "    res[\"choices\"][0][\"message\"],\n",
    "    {\"content\": MORE_REQUEST_PROMPT, \"role\": \"user\"},\n",
    "]\n",
    "messages = messages + added_messages\n",
    "\n",
    "res = test_solar_json_mode(messages=messages)\n",
    "data = json.loads(res[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print json pretty with double quotes\n",
    "dumps = json.dumps(data, indent=2)\n",
    "print(dumps, file=open(\"share_2.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_messages = [\n",
    "    res[\"choices\"][0][\"message\"],\n",
    "    {\"content\": MORE_REQUEST_PROMPT, \"role\": \"user\"},\n",
    "]\n",
    "messages = messages + added_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_solar_json_mode(messages=messages)\n",
    "data = json.loads(res[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'statement': \"The unemployment rate is the lowest it's ever been in the past 50 years.\",\n",
       "  'tags': ['Verifiable', 'Misleading', 'Viral']},\n",
       " {'statement': 'In contrast, the stock market is bad.',\n",
       "  'tags': ['Verifiable', 'Misleading', 'Significant', 'Curiosity']},\n",
       " {'statement': \"Solar Mini is the best model you'll find under $30.\",\n",
       "  'tags': ['Verifiable', 'Misleading', 'Significant', 'Viral']},\n",
       " {'statement': 'The most advantageous aspect of smaller models is their reduced computational time, which enhances responsiveness and efficiency.',\n",
       "  'tags': ['Verifiable', 'Significative']},\n",
       " {'statement': \"Their compact size allows for on-device deployment, enabling a decentralized approach that brings AI capabilities directly to the user's local device.\",\n",
       "  'tags': ['Verifiable', 'Significative']},\n",
       " {'statement': 'Solar Mini delivered outstanding performance in comparison to competitors such as Llama2, Mistral 7B, Ko-Alpaca, and KULLM in various benchmarks.',\n",
       "  'tags': ['Verifiable', 'Misleading', 'Significative', 'Viral']},\n",
       " {'statement': 'Depth up-scaling (DUS) is a method that allows for a more straightforward and efficient enlargement of smaller models compared to other scaling methods like mixture-of-experts.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Depth up-scaling (DUS) does not require complex changes and does not need additional modules or dynamism.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Depth up-scaling (DUS) is compatible with easy-to-use LLM frameworks such as HuggingFace and is applicable to all transformer architectures.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Solar Mini is particularly compatible with RAG systems and benefits from LLMs size increase.',\n",
       "  'tags': ['Verifiable', 'Significative']},\n",
       " {'statement': 'Solar Mini is designed to follow instructions in a QA format during the instruction tuning stage.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Solar Mini is trained to align with human or powerful AI preferences during the alignment tuning stage.',\n",
       "  'tags': ['Verifiable']},\n",
       " {'statement': 'Solar Mini can be used with high-end components to enhance its performance and reliability.',\n",
       "  'tags': ['Verifiable', 'Significative']},\n",
       " {'statement': 'Solar Mini is compatible with a variety of file formats, including PDF, PNG, and JPG, through its OCR and Layout Analysis module.',\n",
       "  'tags': ['Verifiable', 'Significative']},\n",
       " {'statement': 'Solar Mini is designed to extract tables and figures from any document using OCR and Layout Analysis.',\n",
       "  'tags': ['Verifiable', 'Significative']}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
