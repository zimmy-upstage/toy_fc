{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"sample\")\n",
    "all_files = [\"solar-mini.txt\", \"finetune.txt\", \"proofreading.txt\"]\n",
    "target_file = all_files[2]\n",
    "\n",
    "with open(data_dir / target_file, \"r\") as f:\n",
    "    blog = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage as Chat\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "MODEL_NAME = \"solar-pro\"\n",
    "llm = Chat(model=MODEL_NAME)\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0\"\n",
    "}\n",
    "ddg_search = DuckDuckGoSearchResults(headers=headers)\n",
    "confidence_threshold = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 13 claimed facts completed.\n"
     ]
    }
   ],
   "source": [
    "from fc import extracted_claimed_facts\n",
    "max_num_try = 10\n",
    "\n",
    "for attempt in range(max_num_try):\n",
    "    try:\n",
    "        claimed_facts = extracted_claimed_facts(blog)\n",
    "        print(f\"Extracted {len(claimed_facts)} claimed facts completed.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        if attempt < max_num_try - 1:\n",
    "            print(f\"Retrying... (Attempt {attempt + 2}/{max_num_try})\")\n",
    "        else:\n",
    "            print(\"Max retries reached. Extraction failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: https://links.duckduckgo.com/d.js?q=Upstage+Solar+LLM+customization+Solar-Mini+fine-tuning+Predibase+proofreading+media+company+large+LLMs+shortcomings+Solar-Proofread+accuracy+cost+effectiveness+automation+enterprise+AI+South+Korea+AI+company+Samsung+AIA+Hyundai+GPT-4+GPT-4o+mini+OpenAI+models+article+proofreading+reduced+operational+costs&kl=wt-wt&l=wt-wt&p=&s=0&df=y&vqd=4-40299537339027023641020061961472942235&bing_market=wt-WT&ex=-1 202 Ratelimit\n",
      "Retrying... (Attempt 2/10)\n",
      "Search context completed.\n"
     ]
    }
   ],
   "source": [
    "from search import search_context\n",
    "\n",
    "for attempt in range(100):\n",
    "    try:\n",
    "        context = search_context(blog, claimed_facts, ddg_search)\n",
    "        print(\"Search context completed.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        if attempt < max_num_try - 1:\n",
    "            print(f\"Retrying... (Attempt {attempt + 2}/{max_num_try})\")\n",
    "        else:\n",
    "            print(\"Max retries reached. Search context failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge graph built.\n"
     ]
    }
   ],
   "source": [
    "from fc import build_kg\n",
    "\n",
    "for attempt in range(max_num_try):\n",
    "    try:\n",
    "        kg = build_kg(claimed_facts, context, llm)\n",
    "        print(\"Knowledge graph built.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        if attempt < max_num_try - 1:\n",
    "            print(f\"Retrying... (Attempt {attempt + 2}/{max_num_try})\")\n",
    "        else:\n",
    "            print(\"Max retries reached. Knowledge graph building failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts verification completed.\n"
     ]
    }
   ],
   "source": [
    "from fc import verify_facts\n",
    "\n",
    "for attempt in range(max_num_try):\n",
    "    try:\n",
    "        verified_facts = verify_facts(claimed_facts, context, kg, confidence_threshold, llm)\n",
    "        print(\"Facts verification completed.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        if attempt < max_num_try - 1:\n",
    "            print(f\"Retrying... (Attempt {attempt + 2}/{max_num_try})\")\n",
    "        else:\n",
    "            print(\"Max retries reached. Facts verification failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'claimed': \"Upstage is South Korea's most successful AI company, providing state-of-the-art enterprise AI and LLM solutions to thousands of customers like Samsung, AIA, Hyundai, and many more.\",\n",
       "  'Rating': 'HALF TRUE',\n",
       "  'confidence': '0.70',\n",
       "  'explanation': \"While Upstage is a successful AI company providing LLM solutions to notable clients like Samsung, the knowledge graph does not provide information comparing Upstage's success to other AI companies in South Korea, making the claim 'most successful' only partially true.\"},\n",
       " '1': {'claimed': 'Having raised over $200 million since their founding in 2020, they are by far the most-funded tech company in South Korean history.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': '1.0',\n",
       "  'explanation': \"The knowledge graph does not provide any information about Upstage's funding history or comparison to other South Korean tech companies, making the claim inaccurate.\"},\n",
       " '2': {'claimed': 'They have customized their flagship LLM–Solar–to be superior to GPT-4 and all other large models in every aspect, including accuracy, speed, and cost effectiveness.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': '0.9',\n",
       "  'explanation': 'The knowledge graph only mentions that Upstage is an AI company specializing in building powerful LLMs, but it does not provide any information about their flagship LLM-Solar or its comparison to GPT-4 or other large models in terms of accuracy, speed, and cost-effectiveness. Therefore, the claim cannot be verified as true.'},\n",
       " '3': {'claimed': 'A major international media company was struggling under the weight of proofreading over 60 articles per day.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': '1.0',\n",
       "  'explanation': 'The knowledge graph does not mention any media company or proofreading tasks. It only describes Upstage as an AI company specializing in LLMs for enterprises.'},\n",
       " '4': {'claimed': 'Upstage used Predibase to demonstrate that fine-tuning their Solar-Mini model could create a model designed to accurately catch typos and grammatical errors that even the most dedicated human might overlook.',\n",
       "  'Rating': 'MOSTLY TRUE',\n",
       "  'confidence': '0.8',\n",
       "  'explanation': 'The claim is mostly true, as Upstage is confirmed to use Predibase for fine-tuning their proprietary model. However, the specific purpose of catching typos and grammatical errors, and the comparison to human performance, are not explicitly mentioned in the knowledge graph.'},\n",
       " '5': {'claimed': 'Large, off-the-shelf LLMs like GPT-4 and Claude 3.5 Sonnet are completely unsuitable for any enterprise use due to the high cost, long deployment time, and lack of domain and task-specific knowledge.',\n",
       "  'Rating': 'MOSTLY FALSE',\n",
       "  'confidence': '0.8',\n",
       "  'explanation': 'The claim overlooks the existence of companies like Upstage that specialize in building powerful LLMs for enterprises, suggesting that large LLMs can be suitable for enterprise use. However, the claim may still hold true for some general-purpose LLMs, and the provided knowledge graph does not directly address the other aspects of the claim, such as cost and deployment time.'},\n",
       " '6': {'claimed': 'The media company experimented with using popular LLMs such as OpenAI’s models to meet their proofreading needs and they could not accurately pinpoint errors and were prohibitively expensive at scale.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 0.95,\n",
       "  'explanation': \"The provided knowledge graph does not mention the media company, nor does it discuss their experimentation with LLMs, proofreading needs, or the accuracy and cost issues claimed. The graph only references Upstage's expertise in building LLMs.\"},\n",
       " '7': {'claimed': 'Solar-Mini is Upstage’s SLM that delivers responses comparable to GPT-3.5, but 2.5 times faster.',\n",
       "  'Rating': 'MOSTLY TRUE',\n",
       "  'confidence': '0.7',\n",
       "  'explanation': \"The knowledge graph states that Upstage specializes in building powerful LLMs, which aligns with the claim about Solar-Mini being an SLM comparable to GPT-3.5. However, the knowledge graph does not provide information about Solar-Mini's speed, making it impossible to verify the 2.5 times faster claim.\"},\n",
       " '8': {'claimed': 'Solar-Proofread outperformed both the base and a fine-tuned version of GPT-4o mini on edit suggestion correctness.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 1.0,\n",
       "  'explanation': 'The knowledge graph does not provide any information about Solar-Proofread, GPT-4o mini, or their performance comparison. The claim is inaccurate and unsubstantiated based on the provided data.'},\n",
       " '9': {'claimed': 'Solar-Proofread was 79% accurate in identifying errors, compared to the fine-tuned GPT-4o mini model (71%) and the GPT-4o mini base model (25%).',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 1.0,\n",
       "  'explanation': 'The knowledge graph and context do not provide any information about Solar-Proofread, its accuracy, or comparisons with any GPT-4o mini models. Therefore, the claim cannot be verified and is considered false.'},\n",
       " '10': {'claimed': 'Solar-Proofread saw the fewest false positives and missing corrections.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 1.0,\n",
       "  'explanation': 'The provided knowledge graph and context do not contain any information about Solar-Proofread, its false positives, or missing corrections. Therefore, the claim cannot be verified as true.'},\n",
       " '11': {'claimed': 'Solar-Proofread saw the fewest redundant corrections.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 1.0,\n",
       "  'explanation': 'The knowledge graph does not provide any information about Solar-Proofread or redundant corrections, making the claim inaccurate.'},\n",
       " '12': {'claimed': 'With the help of Solar-Proofread, the media company completely automated their proofreading process, allowing them to reallocate valuable resources to creating new content and lowering their operational costs in the process.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 1.0,\n",
       "  'explanation': 'The provided knowledge graph does not contain any information about Solar-Proofread, the media company, or the automation of their proofreading process. The graph only provides information about Upstage, an AI company specializing in building LLMs. Therefore, the claim cannot be verified as accurate.'}}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the intermediate results\n",
    "import json\n",
    "\n",
    "output_dir = (Path(\"output\") / target_file).with_suffix(\"\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_dir / \"claimed_facts.json\", \"w\") as f:\n",
    "    json.dump(claimed_facts, f, indent=4)\n",
    "\n",
    "with open(output_dir / \"context.json\", \"w\") as f:\n",
    "    json.dump(context.split('snippet: ')[1:], f, indent=4)\n",
    "\n",
    "with open(output_dir / \"kg.json\", \"w\") as f:\n",
    "    json.dump(kg, f, indent=4)\n",
    "\n",
    "with open(output_dir / \"verified_facts.json\", \"w\") as f:\n",
    "    json.dump(verified_facts, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the files\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"output\")\n",
    "all_files = [\"solar-mini\", \"finetune\", \"proofreading\"]\n",
    "RATING_ORDER = [\"PANTS ON FIRE\", \"FALSE\", \"MOSTLY FALSE\", \"HALF TRUE\", \"MOSTLY TRUE\", \"TRUE\"]\n",
    "\n",
    "for blog in all_files:\n",
    "    for file in output_dir.glob(f\"{blog}/verified_facts.json\"):\n",
    "        with open(file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            data = sorted(data.items(), key=lambda x: RATING_ORDER.index(x[1][\"Rating\"]))\n",
    "            data = [x[1] for x in data]\n",
    "            # save the data\n",
    "            with open(file, \"w\") as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blog in all_files:\n",
    "    for file in output_dir.glob(f\"{blog}/claimed_facts.json\"):\n",
    "        with open(file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            # sort the number of tags\n",
    "            data = sorted(data, key=lambda x: len(x[\"tags\"]), reverse=True)\n",
    "            # save the data\n",
    "            with open(file, \"w\") as f:\n",
    "                json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'claimed': \"Upstage is South Korea's most successful AI company, providing state-of-the-art enterprise AI and LLM solutions to thousands of customers like Samsung, AIA, Hyundai, and many more.\",\n",
       "  'Rating': 'HALF TRUE',\n",
       "  'confidence': '0.70',\n",
       "  'explanation': \"While Upstage is a successful AI company providing LLM solutions to notable clients like Samsung, the knowledge graph does not provide information comparing Upstage's success to other AI companies in South Korea, making the claim 'most successful' only partially true.\"},\n",
       " '1': {'claimed': 'Having raised over $200 million since their founding in 2020, they are by far the most-funded tech company in South Korean history.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': '1.0',\n",
       "  'explanation': \"The knowledge graph does not provide any information about Upstage's funding history or comparison to other South Korean tech companies, making the claim inaccurate.\"},\n",
       " '2': {'claimed': 'They have customized their flagship LLM–Solar–to be superior to GPT-4 and all other large models in every aspect, including accuracy, speed, and cost effectiveness.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': '0.9',\n",
       "  'explanation': 'The knowledge graph only mentions that Upstage is an AI company specializing in building powerful LLMs, but it does not provide any information about their flagship LLM-Solar or its comparison to GPT-4 or other large models in terms of accuracy, speed, and cost-effectiveness. Therefore, the claim cannot be verified as true.'},\n",
       " '3': {'claimed': 'A major international media company was struggling under the weight of proofreading over 60 articles per day.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': '1.0',\n",
       "  'explanation': 'The knowledge graph does not mention any media company or proofreading tasks. It only describes Upstage as an AI company specializing in LLMs for enterprises.'},\n",
       " '4': {'claimed': 'Upstage used Predibase to demonstrate that fine-tuning their Solar-Mini model could create a model designed to accurately catch typos and grammatical errors that even the most dedicated human might overlook.',\n",
       "  'Rating': 'MOSTLY TRUE',\n",
       "  'confidence': '0.8',\n",
       "  'explanation': 'The claim is mostly true, as Upstage is confirmed to use Predibase for fine-tuning their proprietary model. However, the specific purpose of catching typos and grammatical errors, and the comparison to human performance, are not explicitly mentioned in the knowledge graph.'},\n",
       " '5': {'claimed': 'Large, off-the-shelf LLMs like GPT-4 and Claude 3.5 Sonnet are completely unsuitable for any enterprise use due to the high cost, long deployment time, and lack of domain and task-specific knowledge.',\n",
       "  'Rating': 'MOSTLY FALSE',\n",
       "  'confidence': '0.8',\n",
       "  'explanation': 'The claim overlooks the existence of companies like Upstage that specialize in building powerful LLMs for enterprises, suggesting that large LLMs can be suitable for enterprise use. However, the claim may still hold true for some general-purpose LLMs, and the provided knowledge graph does not directly address the other aspects of the claim, such as cost and deployment time.'},\n",
       " '6': {'claimed': 'The media company experimented with using popular LLMs such as OpenAI’s models to meet their proofreading needs and they could not accurately pinpoint errors and were prohibitively expensive at scale.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 0.95,\n",
       "  'explanation': \"The provided knowledge graph does not mention the media company, nor does it discuss their experimentation with LLMs, proofreading needs, or the accuracy and cost issues claimed. The graph only references Upstage's expertise in building LLMs.\"},\n",
       " '7': {'claimed': 'Solar-Mini is Upstage’s SLM that delivers responses comparable to GPT-3.5, but 2.5 times faster.',\n",
       "  'Rating': 'MOSTLY TRUE',\n",
       "  'confidence': '0.7',\n",
       "  'explanation': \"The knowledge graph states that Upstage specializes in building powerful LLMs, which aligns with the claim about Solar-Mini being an SLM comparable to GPT-3.5. However, the knowledge graph does not provide information about Solar-Mini's speed, making it impossible to verify the 2.5 times faster claim.\"},\n",
       " '8': {'claimed': 'Solar-Proofread outperformed both the base and a fine-tuned version of GPT-4o mini on edit suggestion correctness.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 1.0,\n",
       "  'explanation': 'The knowledge graph does not provide any information about Solar-Proofread, GPT-4o mini, or their performance comparison. The claim is inaccurate and unsubstantiated based on the provided data.'},\n",
       " '9': {'claimed': 'Solar-Proofread was 79% accurate in identifying errors, compared to the fine-tuned GPT-4o mini model (71%) and the GPT-4o mini base model (25%).',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 1.0,\n",
       "  'explanation': 'The knowledge graph and context do not provide any information about Solar-Proofread, its accuracy, or comparisons with any GPT-4o mini models. Therefore, the claim cannot be verified and is considered false.'},\n",
       " '10': {'claimed': 'Solar-Proofread saw the fewest false positives and missing corrections.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 1.0,\n",
       "  'explanation': 'The provided knowledge graph and context do not contain any information about Solar-Proofread, its false positives, or missing corrections. Therefore, the claim cannot be verified as true.'},\n",
       " '11': {'claimed': 'Solar-Proofread saw the fewest redundant corrections.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 1.0,\n",
       "  'explanation': 'The knowledge graph does not provide any information about Solar-Proofread or redundant corrections, making the claim inaccurate.'},\n",
       " '12': {'claimed': 'With the help of Solar-Proofread, the media company completely automated their proofreading process, allowing them to reallocate valuable resources to creating new content and lowering their operational costs in the process.',\n",
       "  'Rating': 'FALSE',\n",
       "  'confidence': 1.0,\n",
       "  'explanation': 'The provided knowledge graph does not contain any information about Solar-Proofread, the media company, or the automation of their proofreading process. The graph only provides information about Upstage, an AI company specializing in building LLMs. Therefore, the claim cannot be verified as accurate.'}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
